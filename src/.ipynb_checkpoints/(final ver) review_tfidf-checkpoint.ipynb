{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of businesses: 2051\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocess import JSONLoader\n",
    "fields = ['business_id']\n",
    "city = ['Toronto']\n",
    "categories = ['Burgers','Seafood','Italian','Chinese','Japanese']\n",
    "\n",
    "business = 'business.json'\n",
    "review = 'review.json'\n",
    "data_dir = 'data/raw'\n",
    "\n",
    "jl = JSONLoader(business, data_dir, fields = fields, encoding = 'utf-8')\n",
    "jl.set_condition(city=city, categories=categories)\n",
    "f_b, business_id = jl.sample(10000000)\n",
    "business_id = set([i[0] for i in business_id])\n",
    "print('The number of businesses: %i' % len(business_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reviews: 96936\n"
     ]
    }
   ],
   "source": [
    "fields = ['business_id','text']\n",
    "jl = JSONLoader(review, data_dir, fields = fields)\n",
    "jl.set_condition(business_id = business_id)\n",
    "f_, rv = jl.sample(10000000)\n",
    "print('Total number of reviews: %i' % len(rv)) # The total number of review texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the TF-IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(19171 unique tokens: ['smashed', 'burger', 'done', 'properly', 'heart']...)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import wordpunct_tokenize\n",
    "import pickle\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "# remove common words and tokenize\n",
    "documents = [r[-1] for r in rv]\n",
    "l = WordNetLemmatizer()\n",
    "texts = [[l.lemmatize(word).lower() for word in wordpunct_tokenize(document)\n",
    "          if word not in stops and word.lower() not in stops] for document in documents]\n",
    "\n",
    "# use the words that appears at least 5 times\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 5] for text in texts]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in texts]\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "# Save\n",
    "with open('./data/tfidf_dictionary', 'wb') as f:\n",
    "    pickle.dump(dictionary, f)\n",
    "with open('./data/tfidf_model', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 0.009567939006763143),\n",
       " (15, 0.17553062693704843),\n",
       " (67, 0.07845205361880085),\n",
       " (71, 0.04842777718301017),\n",
       " (79, 0.03400475771086676),\n",
       " (83, 0.09400398480219363),\n",
       " (103, 0.18256978114525738),\n",
       " (154, 0.12641186621990233),\n",
       " (173, 0.11263122890774996),\n",
       " (190, 0.06943477493147623),\n",
       " (194, 0.08580686193168809),\n",
       " (200, 0.06859922188742085),\n",
       " (219, 0.17502463441017685),\n",
       " (261, 0.1190168271379765),\n",
       " (273, 0.0831025697114241),\n",
       " (298, 0.20023219261938524),\n",
       " (299, 0.20479590417924645),\n",
       " (306, 0.18898107831723246),\n",
       " (324, 0.14749251085289328),\n",
       " (351, 0.12394813451953766),\n",
       " (364, 0.09319522474716169),\n",
       " (377, 0.1414989059679592),\n",
       " (597, 0.10658468032178077),\n",
       " (601, 0.10895111746218249),\n",
       " (626, 0.12069712125800734),\n",
       " (661, 0.12440289616146395),\n",
       " (689, 0.12967026164077458),\n",
       " (696, 0.1354249917052643),\n",
       " (700, 0.144770859493165),\n",
       " (855, 0.17278932296947688),\n",
       " (967, 0.14418150900767712),\n",
       " (1017, 0.12520564316441013),\n",
       " (1041, 0.24073491659736185),\n",
       " (1122, 0.181230637297361),\n",
       " (1127, 0.20550528416547523),\n",
       " (1164, 0.18776670063401432),\n",
       " (1184, 0.15430007140453778),\n",
       " (1265, 0.1908247842117238),\n",
       " (1478, 0.17990763690023945),\n",
       " (1843, 0.181230637297361),\n",
       " (1943, 0.21136313527164416),\n",
       " (3249, 0.21423633777407297),\n",
       " (4922, 0.27647669597786234)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./data/tfidf_dictionary', 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n",
    "with open('./data/tfidf_model', 'rb') as f:\n",
    "    tfidf = pickle.load(f)\n",
    "    \n",
    "tfidf[corpus[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.045629315), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0), (9, 0.0), (10, 0.0), (11, 0.0), (12, 0.0), (13, 0.0), (14, 0.0), (15, 0.0), (16, 0.0), (17, 0.042376243), (18, 0.0), (19, 0.0), (20, 0.0), (21, 0.0), (22, 0.0), (23, 0.0), (24, 0.0), (25, 0.0), (26, 0.0), (27, 0.016292443), (28, 0.0), (29, 0.0), (30, 0.0), (31, 0.0), (32, 0.0), (33, 0.0), (34, 0.0), (35, 0.0), (36, 0.0), (37, 0.0), (38, 0.0), (39, 0.0), (40, 0.0), (41, 0.0), (42, 0.019063195), (43, 0.0), (44, 0.0), (45, 0.045981325), (46, 0.14004759), (47, 0.0), (48, 0.0), (49, 0.0), (50, 0.0), (51, 0.0), (52, 0.0), (53, 0.0), (54, 0.078764737), (55, 0.0), (56, 0.0), (57, 0.0068733585), (58, 0.015946809), (59, 0.028438997), (60, 0.0), (61, 0.0), (62, 0.1467528), (63, 0.0), (64, 0.0), (65, 0.0), (66, 0.010729043), (67, 0.0), (68, 0.016546333), (69, 0.011385167), (70, 0.0), (71, 0.0), (72, 0.0), (73, 0.0), (74, 0.0), (75, 0.05114264), (76, 0.0), (77, 0.0), (78, 0.0), (79, 0.16243352), (80, 0.010373205), (81, 0.0), (82, 0.0), (83, 0.0), (84, 0.0), (85, 0.0), (86, 0.0), (87, 0.0), (88, 0.0), (89, 0.0), (90, 0.0), (91, 0.0), (92, 0.0), (93, 0.075928442), (94, 0.0), (95, 0.0), (96, 0.0), (97, 0.0), (98, 0.0), (99, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = 'this chinese restaurant is awesome'\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[corpus[:100]], len(dictionary))\n",
    "sims = index[tfidf[new_vec]] # similarity between new_vec and all other corpus\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build TF-IDF vectors for a specific category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://radimrehurek.com/gensim/tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Takes quite long\n",
    "\n",
    "import pickle\n",
    "categories =['Burgers','Seafood','Italian','Chinese','Japanese']\n",
    "data_dir = 'data/raw'\n",
    "with open('./data/tfidf_dictionary', 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n",
    "with open('./data/tfidf_model', 'rb') as f:\n",
    "    tfidf = pickle.load(f)    \n",
    "\n",
    "def get_category_doc(category, data_dir, dictionary):\n",
    "    from utils.preprocess import JSONLoader\n",
    "    fields = ['business_id']\n",
    "    city = ['Toronto']\n",
    "    categories = [category]\n",
    "    business = 'business.json'\n",
    "    review = 'review.json'\n",
    "\n",
    "    jl = JSONLoader(business, data_dir, fields = fields, encoding = 'utf-8')\n",
    "    jl.set_condition(city=city, categories=categories)\n",
    "    f_b, business_id = jl.sample(10000000)\n",
    "    business_id = set([i[0] for i in business_id])\n",
    "    fields = ['business_id','text']\n",
    "    jl = JSONLoader(review, data_dir, fields = fields)\n",
    "    jl.set_condition(business_id = business_id)\n",
    "    f_, rv = jl.sample(10000000)\n",
    "    \n",
    "    from itertools import chain\n",
    "    rv_flat = list(chain(*[r[-1].lower().split() for r in rv]))\n",
    "    doc = dictionary.doc2bow(rv_flat)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "doc_cat = [get_category_doc(cat, data_dir, dictionary) for cat in categories]\n",
    "\n",
    "with open('./data/category_doc', 'wb') as f:\n",
    "    pickle.dump(doc_cat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burgers 0.0103107\n",
      "Seafood 0.0192852\n",
      "Italian 0.0131561\n",
      "Chinese 0.0581643\n",
      "Japanese 0.0134848\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('./data/category_doc', 'rb') as f:\n",
    "    doc_cat = pickle.load(f)\n",
    "new_doc = 'this chinese restaurant is awesome'\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "\n",
    "def cosine_sim(v, doc_cat, tfidf, dictionary):\n",
    "    from gensim import similarities\n",
    "    index = similarities.SparseMatrixSimilarity(tfidf[doc_cat], len(dictionary))\n",
    "    sims = index[tfidf[v]] # similarity between v and all other corpus\n",
    "    \n",
    "    return sims\n",
    "\n",
    "categories = ['Burgers','Seafood','Italian','Chinese','Japanese']\n",
    "for i, s in enumerate(cosine_sim(new_vec, doc_cat, tfidf, dictionary)):\n",
    "    print(categories[i], s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build TF-IDF vectors for each businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def get_business_doc(data_dir, dictionary):\n",
    "    '''\n",
    "    Returns:\n",
    "        business_doc (dict): Dictionary of (business_id, tf-idf vector).\n",
    "        The tf-idf vector is calculated for the reviews for the businesses in Toronto,\n",
    "        and whose categories are one of the 'Burgers','Seafood','Italian','Chinese','Japanese'.\n",
    "    '''\n",
    "    from utils.preprocess import JSONLoader\n",
    "    fields = ['business_id']\n",
    "    city = ['Toronto']\n",
    "    categories = ['Burgers','Seafood','Italian','Chinese','Japanese']\n",
    "    business = 'business.json'\n",
    "    review = 'review.json'\n",
    "\n",
    "    jl = JSONLoader(business, data_dir, fields = fields, encoding = 'utf-8')\n",
    "    jl.set_condition(city=city, categories=categories)\n",
    "    f_b, business_id = jl.sample(10000000)\n",
    "    business_id = set([i[0] for i in business_id])\n",
    "    \n",
    "    # Get the reviews for the businesses in Toronto, and whose\n",
    "    # categories are 'Burgers','Seafood','Italian','Chinese','Japanese'\n",
    "    fields = ['business_id','text']\n",
    "    jl = JSONLoader(review, data_dir, fields = fields)\n",
    "    jl.set_condition(business_id = business_id)\n",
    "    f_, rv = jl.sample(10000000)\n",
    "    \n",
    "    # dicionary of business, docs pair\n",
    "    from collections import defaultdict\n",
    "    business_doc = defaultdict(lambda: [])\n",
    "    for b_id, b_rv in rv:\n",
    "        doc = dictionary.doc2bow(b_rv.lower().split())\n",
    "        business_doc[b_id].extend(doc)\n",
    "    \n",
    "    return business_doc\n",
    "\n",
    "doc_business = get_business_doc(data_dir, dictionary)\n",
    "doc_business = dict(doc_business)\n",
    "\n",
    "with open('./data/business_doc', 'wb') as f:\n",
    "    pickle.dump(doc_business, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the tf-idf between the categories and businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burgers 0.197789\n",
      "Seafood 0.147769\n",
      "Italian 0.139211\n",
      "Chinese 0.137562\n",
      "Japanese 0.139398\n",
      "[['JB8-8TtNYX-vLqN7cz-zHA', ['Burgers', 'Restaurants']]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from gensim import similarities\n",
    "\n",
    "categories =['Burgers','Seafood','Italian','Chinese','Japanese']\n",
    "data_dir = 'data/raw'\n",
    "with open('./data/tfidf_dictionary', 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n",
    "with open('./data/tfidf_model', 'rb') as f:\n",
    "    tfidf = pickle.load(f)    \n",
    "with open('./data/category_doc', 'rb') as f:\n",
    "    doc_cat = pickle.load(f)\n",
    "with open('./data/business_doc', 'rb') as f:\n",
    "    doc_business = pickle.load(f)\n",
    "\n",
    "def cosine_sim(v, doc_cat, tfidf, dictionary):\n",
    "    index = similarities.SparseMatrixSimilarity(tfidf[doc_cat], len(dictionary))\n",
    "    sims = index[tfidf[v]] # similarity between v and all other corpus\n",
    "    \n",
    "    return sims\n",
    "\n",
    "# Test\n",
    "b_id_test = list(doc_business.keys())[0]\n",
    "\n",
    "for i, s in enumerate(cosine_sim(doc_business[b_id_test], doc_cat, tfidf, dictionary)):\n",
    "    print(categories[i], s)\n",
    "\n",
    "from utils.preprocess import JSONLoader\n",
    "fields = ['business_id', 'categories']\n",
    "city = ['Toronto']\n",
    "business_id = [b_id_test]\n",
    "\n",
    "business = 'business.json'\n",
    "data_dir = 'data/raw'\n",
    "\n",
    "jl = JSONLoader(business, data_dir, fields = fields, encoding = 'utf-8')\n",
    "jl.set_condition(city=city, categories=categories, business_id = business_id)\n",
    "_, b = jl.sample(10000000)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sim_by_business(doc_business, cat):\n",
    "    column = categories.index(cat)\n",
    "    sim = dict()\n",
    "    for b_id in doc_business:\n",
    "        sim[b_id] = cosine_sim(doc_business[b_id], [doc_cat[column]], tfidf, dictionary)[0]\n",
    "    \n",
    "    return sorted(sim.items(), key=lambda x: -x[1])\n",
    "\n",
    "import pickle\n",
    "sims = dict()\n",
    "for cat in categories:\n",
    "    sims = sim_by_business(doc_business, cat)\n",
    "    with open('./data/cos_sim_%s' % cat, 'wb') as f:\n",
    "        pickle.dump(sims, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the final tf-idf similarity between the businesses / categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "categories =['Burgers','Seafood','Italian','Chinese','Japanese']\n",
    "sims = dict()\n",
    "for cat in categories:\n",
    "    with open('./data/cos_sim_%s' % cat, 'rb') as f:\n",
    "        sims[cat] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('zgQHtqX0gqMw1nlBZl2VnQ', 3.5928981),\n",
       " ('RtUvSWO_UZ8V3Wpj0n077w', 3.4973285),\n",
       " ('O1TvPrgkK2bUo5O5aSZ7lw', 3.467016),\n",
       " ('f5O7v_X_jCg2itqacRfxhg', 2.9844866),\n",
       " ('fGurvC5BdOfd5MIuLUQYVA', 2.9722822),\n",
       " ('BUcTdN-rNE8urCCQuxSOQA', 2.9080408),\n",
       " ('DE89UdHFMCN6DtYWZuer5A', 2.8275206),\n",
       " ('_xAJZOKBMPOe47p1MphB2w', 2.7320778),\n",
       " ('RwRNR4z3kY-4OsFqigY5sw', 2.4913545),\n",
       " ('OllK5_S-7svgSwbUfx1xYA', 2.4909453)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims['Chinese'][:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
